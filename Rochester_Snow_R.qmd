---
title: "Rochester Weather Patterns Since 1926"
author: "Cole Durham"
format: pdf
editor: visual
---

```{r, message=FALSE, echo=FALSE}

library(dplyr)
library(lubridate)
library(imputeTS)
library(TSA)
library(forecast)
library(tidyverse)
```

## 1. Introduction

In this project, we will explore the trends exhibited in snowfall and maximum temperature data from Rochester, New York, from January of 1926 until October, 2025. The full dataset contains columns for the daily maximum and minimum temperatures, the amount of precipitation, the amount of snowfall, and the total depth of snow measured on the ground.

```{r}
Rochester <- read.csv("Rochester.csv")
head(Rochester)
```

We will convert the columns `TMAX..Degrees.Fahrenheit.` and `SNOW..Inches.` to separate objects and identify any missing values.

```{r}
#Create snowfall set, then rename col
Rochester.snowfall <- Rochester %>% select("Date", "SNOW..Inches.")

Rochester.snowfall <- Rochester %>%
  select(Date, SNOW..Inches.) %>%
  rename(snowfall = SNOW..Inches.)

#Create max temp set, rename col
Rochester.Tmax <- Rochester %>% select("Date", "TMAX..Degrees.Fahrenheit.")

Rochester.Tmax <- Rochester %>%
select(Date, TMAX..Degrees.Fahrenheit.) %>%
rename(tmax = TMAX..Degrees.Fahrenheit.)

#Make Date col a true datetime object
Rochester.snowfall$Date <- as.Date(Rochester.snowfall$Date, format = "%Y-%m-%d")
Rochester.Tmax$Date     <- as.Date(Rochester.Tmax$Date, format = "%Y-%m-%d")

#Make Year, Month, Day Columns: Should've done before splitting
Rochester.snowfall$Month <- month(Rochester.snowfall$Date)
Rochester.snowfall$Year <- year(Rochester.snowfall$Date)
Rochester.snowfall$Day <- day(Rochester.snowfall$Date)


Rochester.Tmax$Month <- month(Rochester.Tmax$Date)
Rochester.Tmax$Year <- year(Rochester.Tmax$Date)
Rochester.Tmax$Day <- day(Rochester.Tmax$Date)


#Determine dates (rows) with snowfall entered as NA
(snow.na <- Rochester.snowfall[is.na(Rochester.snowfall$snowfall), ])

#Determine dates (rows) with max temp entered as NA
(Tmax.na <- Rochester.Tmax[is.na(Rochester.Tmax$tmax),])
```

Unfortunately, we are presented with an immediate issue in the snowfall data: eight consecutive missing values in October, 1929 and almost twenty consecutive missing values in November, 1940. If these were from the warmer months of March-August, we could impute zeros with no hesitation; however, Rochester weather can be unruly in autumn. There are two ways to fix the largest gaps in the data, the first of which is to simply drop the portion of the observed values prior to 1941. An alternative, since standard techniques (e.g. interpolation, sample mean, last-observation-carry-forward) are generally insufficient for such large gaps, is to randomly sample from the same month in adjacent years.

To get a sense of the daily snowfall amounts and maximum temperatures in October and November over the years, we can plot histograms.

```{r}
hist(Rochester.snowfall[grepl("-10-", Rochester.snowfall$Date), ]$snowfall, main = "October Snowfall Amount Frequency, Rochester NY",
     xlab = "Daily Snowfall Amount (inches)")

hist(Rochester.snowfall[grepl("-11-", Rochester.snowfall$Date), ]$snowfall, main = "November Snowfall Amount Frequency, Rochester NY",
     xlab = "Daily Snowfall Amount (inches)")

hist(Rochester.Tmax[grepl("-10-", Rochester.Tmax$Date), ]$tmax, main = "October Max Temps, Rochester NY",
     xlab = "Max Temp (deg. F)")

hist(Rochester.Tmax[grepl("-11-", Rochester.Tmax$Date), ]$tmax, main = "November Max Temps, Rochester NY",
     xlab = "Max Temp (deg. F)")
```

From the histograms for snowfall in October and November, it would not be unreasonable to simply enter zeros for all missing days, but we will take the extra step to sample from the same month in adjacent years.

## 2. Data Cleaning and Organization

The largest gaps in the snowfall data occur in October, 1929 and November, 1940. These values will be imputed by sampling with replacement from the same month in adjacent years.

```{r}
#Identify NA rows from October, 1929
oct29_idx <- which(
  Rochester.snowfall$Month == 10 &
  Rochester.snowfall$Year == 1929 &
  is.na(Rochester.snowfall$snowfall)
)

#Identify NA rows from November, 1940
nov40_idx <- which(
  Rochester.snowfall$Month == 11 &
  Rochester.snowfall$Year == 1940 &
  is.na(Rochester.snowfall$snowfall)
)

#Gather same months from adjacent years (Oct 1927/30, Nov 1939/41)
adj_oct <- Rochester.snowfall %>%
filter(format(Date, "%Y-%m") %in% c("1927-10", "1930-10")) %>%
filter(!is.na(snowfall)) %>%
pull(snowfall)  
  
adjacent_nov <- Rochester.snowfall %>%
filter(format(Date, "%Y-%m") %in% c("1939-11", "1941-11")) %>%
filter(!is.na(snowfall)) %>%
pull(snowfall)

#Set seed to reproduce; sample with replacement from the adjacent yr numeric vectors
set.seed(42) 
Rochester.snowfall$snowfall[oct29_idx] <- sample(
  adj_oct,
  size = length(oct29_idx),
  replace = TRUE
)

Rochester.snowfall$snowfall[nov40_idx] <- sample(
  adjacent_nov,
  size = length(nov40_idx),
  replace = TRUE
)

#Check for NA again; should be greatly reduced
(snow.na.1 <- Rochester.snowfall[is.na(Rochester.snowfall$snowfall), ])

```

Now the missing values from the month of July in 1932 and 1996 will be imputed as 0.

```{r}
#Identify NA rows from July, 1996
jul96_idx <- which(
  Rochester.snowfall$Month == 7 &
  Rochester.snowfall$Year == 1996 &
  is.na(Rochester.snowfall$snowfall)
)

jul32_idx <- which(
  Rochester.snowfall$Month == 7 &
  Rochester.snowfall$Year == 1932 &
  is.na(Rochester.snowfall$snowfall)
)

#Replace
Rochester.snowfall$snowfall[jul96_idx] <- 0
Rochester.snowfall$snowfall[jul32_idx] <- 0

#Another check of remaining NA rows
(snow.na.1 <- Rochester.snowfall[is.na(Rochester.snowfall$snowfall), ])

```

The remaining two missing data points will be filled according to LOCF (last-observed-carry-forward) method.

```{r}
feb25_26_idx <- which(
  Rochester.snowfall$Month == 2 &
  Rochester.snowfall$Year == 1926 &
  Rochester.snowfall$Day == 25 &
  is.na(Rochester.snowfall$snowfall)
)

feb24_26_idx <- which(
  Rochester.snowfall$Month == 2 &
  Rochester.snowfall$Year == 1926 &
  Rochester.snowfall$Day == 24)

nov14_47_idx <- which(
  Rochester.snowfall$Month == 11 &
  Rochester.snowfall$Year == 1947 &
  Rochester.snowfall$Day == 14 &
  is.na(Rochester.snowfall$snowfall)
)

nov13_47_idx <- which(
  Rochester.snowfall$Month == 11 &
  Rochester.snowfall$Year == 1947 &
  Rochester.snowfall$Day == 13
)

Rochester.snowfall$snowfall[feb25_26_idx] <- Rochester.snowfall$snowfall[feb24_26_idx]
Rochester.snowfall$snowfall[nov14_47_idx] <- Rochester.snowfall$snowfall[nov13_47_idx]

#Hopefully, final check
(snow.na.2 <- Rochester.snowfall[is.na(Rochester.snowfall$snowfall), ])

```

The snowfall data has been successfully cleaned. Now we will look to address the missing maximum temperature entries. In order to get a sense of what imputation method will work best, we need to zoom in on the days near July 17, 1932, and February 20, 1943.

```{r}
par(mfrow = c(2,1), mar = c(2,3,1,1))
missing_temp_32 <- Rochester.Tmax[Rochester.Tmax$Date >= as.Date("1932-07-10") & 
                                Rochester.Tmax$Date <= as.Date("1932-07-24"), ]
missing_temp_43 <- Rochester.Tmax[Rochester.Tmax$Date >= as.Date("1943-02-13") & 
                                Rochester.Tmax$Date <= as.Date("1943-02-27"), ]
plot(missing_temp_32$tmax, main="Maximum Temperatures 07/10/32-07/24/32",
     ylab = "Max Temp")
plot(missing_temp_43$tmax, main="Maximum Temperatures 02/13/43-02/27/43",
     ylab = "Max Temp")
```

The best available option is to use a spline to interpolate and impute. This is automated using the `imputeTS` package. We will impute and look locally at the values to make sure everything is reasonably done before imputing in the actual series.

```{r}
par(mfrow=c(2,1), mar = c(2,4,1,1))
#Spline interps
missing_temp_32$tmax_interp <- na_interpolation(missing_temp_32$tmax, option = "spline")
missing_temp_43$tmax_interp <- na_interpolation(missing_temp_43$tmax, option = "spline")


ggplot_na_imputations(x_with_imputations = missing_temp_32$tmax_interp,
                      x_with_na = missing_temp_32$tmax,
                      title = "Max Temp Impute via Spline: July, 1932",
                  xlab = "Time",
                  ylab = "Max Temp")

ggplot_na_imputations(x_with_imputations = missing_temp_43$tmax_interp,
                      x_with_na = missing_temp_43$tmax,
                      title = "Max Temp Impute via Spline: February, 1943",
                  xlab = "Time",
                  ylab = "Max Temp")
```

These are reasonable visually, though this is a nebulous assessment at best. We will impute using this method.

```{r}
#Replace NA temp values via spline interpolation
Rochester.Tmax$tmax <- na_interpolation(Rochester.Tmax$tmax, option = "spline")
(Tmax.na1 <- Rochester.Tmax[is.na(Rochester.Tmax$tmax),])
```

Now we will compile our time series to contain the following information (noting we do not have January 1st, 1926):

-   Average monthly maximum temperature

-   Total annual snowfall deviation: compare total annual snowfall for each year to the yearly average across 1991-2020. In particular, take $S_i-R_S$ where $T_i$ is the annual total snowfall for year $i$ and $R_S$ is the reference snowfall. This quantity will be non-negative when at least as much snow fell in year $i$ compared to the reference amount, and negative otherwise.

-   Annual maximum temperature deviation: compare the average maximum temperature over a given year to the average maximum temperature across 1991-2020. In particular, take $T_i-R_T$ where $T_i$ is the average maximum temperature for year $i$ and $R_T$ is the reference temperature. This quantity will be nonnegative when the average maximum temperature in a given year is lower than the reference.

```{r}
#Construct monthly snowfall totals
monthly_snowfall <- Rochester.snowfall %>%
  group_by(Year, Month) %>%
  summarize(
    tot_snow = sum(snowfall, na.rm = TRUE)
  )

#Construct average monthly max temp
monthly_tmax <- Rochester.Tmax %>%
  group_by(Year, Month) %>%
  summarize(
    mean_tmax = mean(tmax, na.rm = TRUE)
  )

#Calculate yearly snowfall totals from 1991-2020
monthly_snow_9120 <- Rochester.snowfall[Rochester.snowfall$Year >= 1991 & 
                                Rochester.snowfall$Year <= 2020, ]
annual_snow_9120 <- monthly_snow_9120 %>%
  group_by(Year) %>%
  summarize(
    tot_snow = sum(snowfall)
  )

#Reference annual snowfall amount: average from 1991-2020
annual_snow_reference <- mean(annual_snow_9120$tot_snow)

#Reference temp: average maximum temp over 1991-2020
ref_temp <- Rochester.Tmax %>%
  filter(Year >= 1991, Year <= 2020) %>%
  summarize(ref_temp = mean(tmax, na.rm = TRUE)) %>%
  pull(ref_temp)


#Construct annual snowfall totals
annual_snow <- monthly_snowfall %>%
  group_by(Year) %>%
  summarize(
    tot_snow = sum(tot_snow)
  )

#Construct average annual max temperatures
annual_tmax <- Rochester.Tmax %>%
  group_by(Year) %>%
  summarize(
    tmax_avg = mean(tmax)
  )

#Construct annual snowfall deviation: annual snowfall total - annual_snow_reference
annual_snow_deviations <- annual_snow
annual_snow_deviations$tot_snow <- annual_snow_deviations$tot_snow - annual_snow_reference

#Construct annual temperature deviations
annual_tmax_deviations <- annual_tmax
annual_tmax_deviations$tmax_avg <- annual_tmax_deviations$tmax_avg - ref_temp
```

Before making proper time series objects, we have to make sure the data is ordered properly by year and month.

```{r}
monthly_tmax <- monthly_tmax[order(monthly_tmax$Year, monthly_tmax$Month), ]
annual_snow_deviations <- annual_snow_deviations[order(annual_snow_deviations$Year), ]
```

Finally, our time series objects can be created. The average monthly maximum temperature series will be limited to January, 1950 and beyond.

```{r}
par(mfrow=c(3,1), mar = c(2,4,1,1))

#Restrict max temp average by month to Jan 1950 - Sep 2025
max_temps <- ts(monthly_tmax$mean_tmax, start = c(1926, 1), frequency = 12)
max_temps <- window(max_temps, start = c(1950, 1), end = c(2025, 9))

snowfall_devs <- ts(annual_snow_deviations$tot_snow, start = 1926, frequency = 1)
max_tempdevs <- ts(annual_tmax_deviations$tmax_avg, start = 1926, frequency = 1)

ts.plot(max_temps, ylab = "Temp (deg F)", main = "Rochester Average Monthly Maximum Temp")
ts.plot(window(snowfall_devs, start = 1926), ylab = "Snowfall Deviation (in.)", main = "Rochester Snowfall Compared to 1991-2020 Average")
ts.plot(window(max_tempdevs, start = 1926), ylab = "Temp (deg F)", main = "Rochester Max Temp Deviations from 1991-2020 Average")
```

By construction, the latter two time series are more similar in nature to each other than to the first. The seasonal nature of the maximum temperature averaged over each month provides us with a different sort of forecasting opportunity, which we explore in the next section.

## 3. Analysis of Average Monthly Maximum Temperature

Our main goal in modeling the average monthly maximum temperature is to be able to forecast future movement of the series due to trend and seasonal factors.

### 3.1 Preliminary Analysis

The time series `max_temps` exhibits regular seasonal behavior with stable amplitude and limited discernible trend. We will examine these claims more closely, beginning with two different methods of trend estimation.

One nonparametric method we may use to estimate trend to avoid assuming any functional form is Season-Trend LOESS, a form of detrending via local regression.

```{r}
stl_detrending <- stl(max_temps, s.window = "periodic", t.window = 241)
plot(stl_detrending, main="Detrending via STL")
```

We use a window of length 241 months to extract the trend over approximately twenty year rolling periods, as this was the minimum window size at which predominantly low-frequency movement was captured. A trend is present which accounts for a roughly 2 degree Fahrenheit swing between a minimum of approximately 57 degrees in the early 1960's and a maximum of 59 degrees in 2024. This is consistent with long-term regional climate change.

To confirm that the above process leaves a remainder which resembles white noise, i.e. temporal dependence has been removed, we can inspect the ACF and PACF.

```{r}
par(mfrow=c(2,1), mar = c(4,3,2.5,2))
acf(stl_detrending$time.series[,"remainder"], main = "ACF, PACF of Remainder", lag.max = 36)
pacf(stl_detrending$time.series[,"remainder"], main="", lag.max = 36)

```

The sample ACF does not have consistent significant spikes at positive lags within a three year window, suggesting the remainder does resemble a white noise process. Our next step is to create a baseline model by using polynomial regression to model the trend component and monthly indicator variables to model the seasonal component.

### 3.2 Baseline Model

To train and then evaluate our initial model, we will create a training set from January, 1950 until December, 2024, and a forecast set from January, 2025 until September, 2025.

```{r}
#Create train/forecast sets from actual data and output from stl().
motemps.train <- window(max_temps, start = c(1950, 1), end = c(2024,12))
motemps.fore <- window(max_temps, start = c(2025,1), end = c(2025, 9))

motemps_trend.train <- window(stl_detrending$time.series[,"trend"],
                              start = c(1950, 1), end = c(2024,12))
motemps_trend.fore <- window(stl_detrending$time.series[,"trend"],
                              start = c(2025,1), end = c(2025, 9))

motemps_seas.train <- window(stl_detrending$time.series[,"seasonal"],
                              start = c(1950, 1), end = c(2024,12))
motemps_seas.fore <- window(stl_detrending$time.series[,"seasonal"],
                              start = c(2025,1), end = c(2025, 9))

n_ttrain <- length(motemps.train)
n_tfore <- length(motemps.fore)

#Training time data
ttrain_trend_df <- tibble(
  t_fit = 1:n_ttrain,
  t_sqfit = t_fit^2,
  t_cubefit = t_fit^3
)

#Forecasting time data
tfore_trend_df <- tibble(
  t_fit = (n_ttrain+1):(n_ttrain+n_tfore),
  t_sqfit = t_fit^2,
  t_cubefit = t_fit^3
)

mlr.line <- lm(motemps_trend.train ~ t_fit, data = ttrain_trend_df)
mlr.quadr <- lm(motemps_trend.train ~ t_fit + t_sqfit, data = ttrain_trend_df)
mlr.cubic <- lm(motemps_trend.train ~ t_fit + t_sqfit + t_cubefit, data = ttrain_trend_df)
```

```{r}
trend_pred_quad <- cbind(motemps_trend.train, mlr.line$fitted, mlr.quadr$fitted, mlr.cubic$fitted)
ts.plot(trend_pred_quad, col=c("black", "red", "purple", "blue"), lty = c(1,2,4,6), main = "Estimating Trend with Polynomials", lwd = c(1.5,1.25,1.25,1.25))

legend("topleft",
       legend = c("STL trend", "Linear", "Quadratic", "Cubic"),
       col = c("black", "red", "purple", "blue"),
       lty = c(1, 2, 4, 6),
       lwd = 2,
       bty = "n")
```

It is visually evident that the linear model is too simple to capture the growth pattern while the quadratic and cubic models have comparable performance. These statements are supported by the model summaries:

```{r}
summary(mlr.line)
summary(mlr.quadr)
summary(mlr.cubic)
```

Retaining only the quadratic and cubic models for consideration, we wish to compare their performance on the out-of-sample data. The plots give the initial impression that the actual trend of the data grows too quickly for the quadratic model to keep pace, while the cubic model does a more reasonable job.

```{r, echo=FALSE}
# Use predict function 
#pfore.lin  <- predict(mlr.line,  newdata = tfore_trend_df,       se.fit = TRUE)
pfore.quad <- predict(mlr.quadr, newdata = tfore_trend_df,     se.fit = TRUE)
pfore.cub  <- predict(mlr.cubic, newdata = tfore_trend_df,   se.fit = TRUE)

#Plot of time series with fits and out-of-sample forecasts
# Linear Trend Model fits and forecasts
#linff <- c(mlr.line$fitted, pfore.lin$fit)

# Quadratic Trend Model fits and forecasts
quadff <- c(mlr.quadr$fitted, pfore.quad$fit)

# Cubic Trend Model fits and forecasts
cubff <- c(mlr.cubic$fitted, pfore.cub$fit)

# Bind observed data and fits+forecasts
#obslin <- cbind(stl_detrending$time.series[,"trend"], linff)
obsquad <- cbind(stl_detrending$time.series[,"trend"], quadff)
obscub <- cbind(stl_detrending$time.series[,"trend"], cubff)

#par(mfrow = c(1,3), mar = c(4, 4, 2.5, 2))
#ts.plot(obslin, main = "Linear Trend Model", 
        #col = c("black", "blue"), ylab = "Temp (F)")
#abline(v = time(stl_detrending$time.series[,"trend"])[n_ttrain], 
       #col = "red", lty = "dashed")

ts.plot(obsquad, main = "Quadratic Trend Model", col = c("black", "blue"),
        ylab="Temp (F)")
abline(v = time(stl_detrending$time.series[,"trend"])[n_ttrain], 
       col = "red", lty = "dashed")

ts.plot(obscub, main = "Cubic Trend Model", col = c("black", "blue"),
        ylab="Temp (F)")
abline(v = time(stl_detrending$time.series[,"trend"])[n_ttrain], 
       col = "red", lty = "dashed")

```

To confirm the superiority of the cubic model, we calculate standard forecast metrics.

```{r}
# Forecast/Prediction errors: Observed - Predicted
efore.quad <- motemps_trend.fore - pfore.quad$fit
efore.cub <- motemps_trend.fore - pfore.cub$fit

# Forecast evaluation criteria
me.quad <- mean(efore.quad)                       # Mean Error
mpe.quad <- 100*(mean(efore.quad/motemps_trend.fore))          # Mean Percent Error
mse.quad <- sum(efore.quad**2)/n_tfore              # Mean Squared Error
mae.quad <- mean(abs(efore.quad))                 # Mean Absolute Error
mape.quad <- 100*(mean(abs((efore.quad)/motemps_trend.fore)))  # Mean Absolute Percent Error 
fec.quad <- data.frame(quadratic = rbind(me.quad, mpe.quad, 
                                      mse.quad, mae.quad, mape.quad), 
                      row.names = c("me", "mpe", "mse", "mae", "mape"))

# Forecast evaluation criteria
me.cube <- mean(efore.cub)                       # Mean Error
mpe.cube <- 100*(mean(efore.cub/motemps_trend.fore))          # Mean Percent Error
mse.cube <- sum(efore.cub**2)/n_tfore              # Mean Squared Error
mae.cube <- mean(abs(efore.cub))                 # Mean Absolute Error
mape.cube <- 100*(mean(abs((efore.cub)/motemps_trend.fore)))  # Mean Absolute Percent Error 
fec.cube <- data.frame(cubic = rbind(me.cube, mpe.cube, 
                                      mse.cube, mae.cube, mape.cube), 
                      row.names = c("me", "mpe", "mse", "mae", "mape"))

round(fec.quad, digits = 4)
round(fec.cube, digits = 4)
```

As expected, the cubic trend model has significantly better forecast performance than the quadratic model. We may also consider information criterion when performing model selection, though our forecasting purposes place more weight on the above metrics.

```{r}
k_2 <- 2
k_3 <- 3

AIC.quad <- AIC(mlr.quadr, k=k_2)
AIC.cubic <- AIC(mlr.cubic, k=k_3)

AIC_vals <- data.frame(AIC = rbind(AIC.quad, 
                                      AIC.cubic), 
                      row.names = c("Quadratic", "Cubic"))
AIC_vals
```

This confirms that the increased complexity of the cubic model has merit. We will move forward with the long-term temperature trend modeled by the polynomial in $t$, measured in months since December, 1949:

$$
T_{trend}(t) = 57.48 - 0.0023t + (8.327\times 10^{-7})t^2+(4.784\times 10^{-9})t^3.
$$
